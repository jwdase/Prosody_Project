{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import setup_project_root\n",
    "setup_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "from langaugedetection.data.length_df_tools import select_array\n",
    "from langaugedetection.data.spectrogram import parse\n",
    "\n",
    "\n",
    "def select_language_time(languages, window):\n",
    "    \"\"\"\n",
    "    Given a list of languages, and their respective time\n",
    "    window loads a .csv containing languages and audio files\n",
    "    correlating to that window. It returns a dictionary with\n",
    "    key : language\n",
    "    item : list of audio files\n",
    "    \"\"\"\n",
    "    lang_to_options = {}\n",
    "\n",
    "    for lang in languages:\n",
    "        path = f\"/om2/user/moshepol/prosody/data/raw_audio/{lang}/custom/length.csv\"\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        lang_to_options[lang] = select_array(df, window)\n",
    "\n",
    "        print(f\"Finished reading: {lang}\")\n",
    "\n",
    "    return lang_to_options\n",
    "\n",
    "\n",
    "def train_test_val_split(lang_dict):\n",
    "    \"\"\"\n",
    "    Returns the indices for how the data should be\n",
    "    split between training and testing - does this with\n",
    "    80% train\n",
    "    10% test\n",
    "    10% validation\n",
    "    \"\"\"\n",
    "\n",
    "    max_length = 1_000_000_000\n",
    "\n",
    "    for lang, item in lang_dict.items():\n",
    "        max_length = min(max_length, len(item))\n",
    "\n",
    "    partition = (max_length // 1_000) * 1_000\n",
    "\n",
    "    # Overide, Make sure to remove when submitting it\n",
    "    partition = 3_000\n",
    "\n",
    "    train = int(partition * 0.8)\n",
    "    test = int(partition * 0.1)\n",
    "\n",
    "    return train, train + test, train + test * 2\n",
    "\n",
    "\n",
    "def check_path(base):\n",
    "    \"\"\"\n",
    "    Clears the folder that we're going to\n",
    "    laod the spectrograms into\n",
    "    \"\"\"\n",
    "    folder = Path(base)\n",
    "\n",
    "    # Checks if the folder exists, and then deletes folder and recreates it\n",
    "    if folder.is_dir():\n",
    "        shutil.rmtree(base)\n",
    "\n",
    "    folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "def split_dataset(files, train, test, val):\n",
    "    \"\"\"\n",
    "    Splits the dataset into it's train, test, val\n",
    "    \"\"\"\n",
    "    train_files = files[0:train]\n",
    "    test_files = files[train:test]\n",
    "    validation_files = files[test:val]\n",
    "\n",
    "    return train_files, test_files, validation_files\n",
    "\n",
    "\n",
    "def save_files(spectrograms, path, batch_size, i):\n",
    "    \"\"\"\n",
    "    Code to save the files,\n",
    "    \"\"\"\n",
    "\n",
    "    def file_name(length, num):\n",
    "        return \"batch_\" + '0' * (length - len(str(num))) + str(num) + '_' + str(batch_size)\n",
    "\n",
    "    torch.save(spectrograms, Path(path + \"/\" + file_name(5, i) + '.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "\n",
    "class AudioFileDataset(Dataset):\n",
    "    def __init__(self, audio_dir, length=88_000, sr=16_000):\n",
    "        self.files = list(audio_dir)\n",
    "        self.target_sr = sr\n",
    "        self.length = length\n",
    "        self.samplers = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "\n",
    "        if sr != self.target_sr:\n",
    "\n",
    "            if sr not in self.samplers.keys():\n",
    "                self.samplers[sr] = torchaudio.transforms.Resample(sr, self.target_sr)\n",
    "                \n",
    "            new_wave = self.samplers[sr](waveform)\n",
    "            return self.pad_waveform(new_wave)\n",
    "\n",
    "        return self.pad_waveform(waveform)\n",
    "\n",
    "    def pad_waveform(self, waveform):\n",
    "        length = waveform.shape[-1]\n",
    "\n",
    "        if length > self.length:\n",
    "            raise TypeError\n",
    "\n",
    "        else:\n",
    "            pad = self.length - length\n",
    "            return torch.nn.functional.pad(waveform, (0, pad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading: en\n",
      "Finished reading: it\n",
      "Finished reading: es\n",
      "Finished reading: de\n"
     ]
    }
   ],
   "source": [
    "languages = [\"en\", \"it\", \"es\", \"de\"]\n",
    "window = \"5.5 - 6.0\"\n",
    "\n",
    "lang_dict = select_language_time(languages, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to Pad and then stack all of our tensors -- once created\n",
    "\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "    Takes in a batch of [1 length] audio files and then stacks them\n",
    "    all audio files must be the same length\n",
    "    '''\n",
    "    group = torch.stack(batch)\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = lang_dict['en'][0:10_000]\n",
    "dataset = AudioFileDataset(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=256, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectrogram_batch(batch, n_fft=2048, hop_length=512):\n",
    "    batch = batch.to('cuda')\n",
    "\n",
    "    specs = torch.stft(\n",
    "        batch.squeeze(1),\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        return_complex=True\n",
    "    )\n",
    "\n",
    "    power = specs.abs() ** 2\n",
    "    db = 10 * torch.log10(torch.clamp(power, min=1e-10))\n",
    "\n",
    "    return db.cpu()  # [B, freq_bins, time_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/om2/user/moshepol/prosody/data/raw_audio/en/spect/range_5_5-6_0/train/\n"
     ]
    }
   ],
   "source": [
    "lang = 'en'\n",
    "use = placement = \"range_\" + window.replace(\".\", \"_\").replace(\" \", \"\")\n",
    "placement = 'train'\n",
    "\n",
    "base = f\"/om2/user/moshepol/prosody/data/raw_audio/{lang}/spect/{use}/{placement}/\"\n",
    "print(base)\n",
    "check_path(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Round: 1\n",
      "Completed Round: 2\n",
      "Completed Round: 3\n",
      "Completed Round: 4\n",
      "Completed Round: 5\n",
      "Completed Round: 6\n",
      "Completed Round: 7\n",
      "Completed Round: 8\n",
      "Completed Round: 9\n",
      "Completed Round: 10\n",
      "Completed Round: 11\n",
      "Completed Round: 12\n",
      "Completed Round: 13\n",
      "Completed Round: 14\n",
      "Completed Round: 15\n",
      "Completed Round: 16\n",
      "Completed Round: 17\n",
      "Completed Round: 18\n",
      "Completed Round: 19\n",
      "Completed Round: 20\n",
      "Completed Round: 21\n",
      "Completed Round: 22\n",
      "Completed Round: 23\n",
      "Completed Round: 24\n",
      "Completed Round: 25\n",
      "Completed Round: 26\n",
      "Completed Round: 27\n",
      "Completed Round: 28\n",
      "Completed Round: 29\n",
      "Completed Round: 30\n",
      "Completed Round: 31\n",
      "Completed Round: 32\n",
      "Completed Round: 33\n",
      "Completed Round: 34\n",
      "Completed Round: 35\n",
      "Completed Round: 36\n",
      "Completed Round: 37\n",
      "Completed Round: 38\n",
      "Completed Round: 39\n",
      "Completed Round: 40\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for batch_waveforms in loader:\n",
    "    specs = compute_spectrogram_batch(batch_waveforms)  # [B, F, T]\n",
    "\n",
    "    save_files(specs, base, len(specs), i)\n",
    "    i += 1\n",
    "\n",
    "    print(f'Completed Round: {i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
